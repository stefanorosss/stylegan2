{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2_colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzA1-mt88AO_"
      },
      "source": [
        "# StyleGAN2 operations\r\n",
        "\r\n",
        "### Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFfx8GQIAQm"
      },
      "source": [
        "Load necessary modules, connect to Google Drive, clone StyleGAN2 repo:  \r\n",
        "*(run this cell again if the session is restarted!)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seuCKI788APF"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get -qq install ffmpeg\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/G', force_remount=True)\n",
        "gdir = !ls /G/\n",
        "gdir = '/G/%s/' % str(gdir[0])\n",
        "%cd $gdir\n",
        "\n",
        "work_name = 'sg2_eps' # change this as you want\n",
        "work_dir = gdir + work_name + '/'\n",
        "if not os.path.isdir(work_dir):\n",
        "  !git clone -b colab git://github.com/eps696/stylegan2 $work_name\n",
        "%cd $work_dir\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS5Eu7o8WEUO"
      },
      "source": [
        "> All directories, mentioned below, are within StyleGAN2 copy on your connected G drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWpWFeyO8APF"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_LGhTfV8APG"
      },
      "source": [
        "First, let's prepare the data. \n",
        "\n",
        "If you work with patterns or shapes (rather than compostions), you can crop square fragments from bigger images (effectively multiplying their amount). Upload your raw images into `data/src` folder, rename `mydata` below according to your needs and run the cell. This will cut the images into `size`px fragments, overlapped with `step` shift by X and Y:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRGJVVY8APG"
      },
      "source": [
        "src_dir = 'data/src'\n",
        "data_dir = 'data/mydata'\n",
        "size = 512\n",
        "step = 256\n",
        "\n",
        "%cd $work_dir\n",
        "%run src/util/multicrop.py --in_dir $src_dir --out_dir $data_dir --size $size --step $step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7XpOCtZhuUo"
      },
      "source": [
        "> If you want to edit input images yourself (e.g. to keep the compositions, or to work with non-square aspect ratios) -- skip the cell above, and upload your prepared data to the directory `data/mydata` (rename as needed):  \r\n",
        "*(run this cell again if the session is restarted!)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZWYW08uhu4q"
      },
      "source": [
        "%cd $work_dir\r\n",
        "data_dir = 'data/mydata'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcQIHRoF8APG"
      },
      "source": [
        "Now let's make compact TFRecords dataset from directory with JPG images `data/mydata`.  \n",
        "This will create `mydata-512x512.tfr` file in `data` directory.  \n",
        "> *For images with alpha channel remove `--jpg` option.*  \n",
        "*For conditional model split the data by subfolders (`mydata/1`, `mydata/2`, ..) and add `--labels` option.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQIWgp478APH"
      },
      "source": [
        "%run src/training/dataset_tool.py --dataset $data_dir --jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfczVF0W8APH"
      },
      "source": [
        "Finally, we can train StyleGAN2 on the prepared dataset:  \n",
        "*(remove `--jpg_data` if your images have alpha channel)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKmktOPb8APH"
      },
      "source": [
        "%run src/train.py --dataset $data_dir --jpg_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYlI1fXe8APH"
      },
      "source": [
        "> This will run training process, according to the options in `src/train.py`. If there was no TFRecords file from the previous step, it will be created at this point. The training results (models and samples) are saved under the `train` directory, similar to original Nvidia approach. There are two types of models saved: compact (containing only Gs network for inference) as `<dataset>-...pkl` (e.g. `mydata-512-0360.pkl`), and full (containing G/D/Gs networks for further training) as `snapshot-...pkl`. \n",
        "\n",
        "> By default, the most powerful SG2 config (F) is used; if you face OOM issue, you may resort to `--config E`, requiring less memory (with poorer results, of course). For small datasets (100x images instead of 10000x) one should add `--d_aug` option to use [Differential Augmentation](https://github.com/mit-han-lab/data-efficient-gans) for more effective training. \n",
        "\n",
        "> The length of the training is defined by `--lod_step_kimg X` argument. It's kind of legacy from [progressive GAN](https://github.com/tkarras/progressive_growing_of_gans) and defines one step of progressive training. Network with base resolution 1024px will be trained for 20 such steps, for 512px - 18 steps, et cetera. Reasonable `lod_step_kimg` value for big datasets is 300-600, while in `--d_aug` mode 20-40 is sufficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpeUmhiH8API"
      },
      "source": [
        "If the training process was interrupted, we can resume it from the last saved model as following:  \n",
        "*(replace `000-mydata-512-f` with existing training directory)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr73IUPG8API"
      },
      "source": [
        "%run src/train.py --dataset $data_dir --jpg_data --resume train/000-mydata-512-f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVxyOjz48API"
      },
      "source": [
        "NB: In most cases it's much easier to use a \"transfer learning\" trick, rather than perform full training from the scratch. For that, we use existing well-trained model as a starter, and \"finetune\" (uptrain) it with our data. This works pretty well, even if our dataset is very different from the original model. \n",
        "\n",
        "So here is a faster way to train our GAN (presuming we have full trained model `train/ffhq-512.pkl` already):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3kwDOh88APJ"
      },
      "source": [
        "%run src/train.py --dataset $data_dir --jpg_data --resume train/ffhq-512.pkl --d_aug --lod_step_kimg 20 --finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V0Yuwef8APJ"
      },
      "source": [
        "There's no need to go for exact steps in this case, you may stop when you're ok with the results. Lower `lod_step_kimg` helps following the progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Njelbgu8APJ"
      },
      "source": [
        "## Generation\n",
        "\n",
        "Let's produce some imagery from the original cat model (download it from [here](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and put to `models` directory).  \n",
        "*(run this cell again if the session is restarted!)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBHD0n0i8APJ"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "%cd $work_dir\n",
        "\n",
        "def makevid(seq_dir):\n",
        "  out_sequence = seq_dir + '/%06d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  return \"\"\"<video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url\n",
        "\n",
        "model = 'models/stylegan2-cat-config-f' # without \".pkl\" extension\n",
        "model_pkl = model + '.pkl' # with \".pkl\" extension\n",
        "output = '_out/cats' # output directory\n",
        "frames = '50-10'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C6BkdFJ8APJ"
      },
      "source": [
        "Generate some animation to test the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcTNtKEF8APJ"
      },
      "source": [
        "%run src/_genSGAN2.py --model $model_pkl --out_dir $output --frames $frames\n",
        "HTML(makevid(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlzqD3DEGzIH"
      },
      "source": [
        "> Here we loaded the model 'as is', and produced 50 frames in its natural resolution, interpolating between random latent space keypoints, with a step of 10 frames between keypoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnWA884M8APK"
      },
      "source": [
        "Now let's generate more custom animation. For that we omit model extension, so the script would load custom network, effectively enabling special features, e.g. arbitrary resolution (set by `--size` argument in `X-Y` format).  \n",
        "`--cubic` option changes linear interpolation to cubic for smoother animation (there is also `--gauss` option for additional smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwoOBOcR8APK"
      },
      "source": [
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --cubic\n",
        "HTML(makevid(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg1-JJYF8APL"
      },
      "source": [
        "> Adding `--save_lat` option will save all traversed dlatent points as Numpy array in `*.npy` file (useful for further curating)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-nyGwL8APL"
      },
      "source": [
        "Generate more various imagery:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOGVIJt18APL"
      },
      "source": [
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 768-256 -n 3-1\n",
        "HTML(makevid(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ3zGyFy8APL"
      },
      "source": [
        "> Here we get animated composition of 3 independent frames, blended together horizontally.  \n",
        "Argument `--splitfine X` controls boundary fineness (0 = smoothest/default, higher => thinner).  \n",
        "\n",
        "Instead of frame splitting, we can load external mask from b/w image file (or folder with image sequence):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB0-VZ798APL"
      },
      "source": [
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --latmask _in/mask.jpg\n",
        "HTML(makevid(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWUJDcGW8APM"
      },
      "source": [
        "`--digress X` adds some funky displacements with X strength (by tweaking initial constant layer).  \n",
        "`--trunc X` controls truncation psi parameter (0 = boring, 1+ = weird). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tITi-ILU8APM"
      },
      "source": [
        "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --digress 2 --trunc 0.5\n",
        "HTML(makevid(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLBZAOSi8APM"
      },
      "source": [
        "### Latent space exploration\n",
        "\n",
        "For these experiments download [FFHQ model](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and save to `models` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB7ujwkD8APM"
      },
      "source": [
        "from IPython.display import HTML, Image\n",
        "from base64 import b64encode\n",
        "%cd $work_dir\n",
        "\n",
        "def makevid(seq_dir, size=512):\n",
        "  out_sequence = seq_dir + '/%06d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  return \"\"\"<video width=%d height=%d controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (size, size, data_url)\n",
        "\n",
        "model = 'models/stylegan2-ffhq-config-f' # without \".pkl\" extension\n",
        "model_pkl = model + '.pkl' # with \".pkl\" extension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIbPEUyI8APN"
      },
      "source": [
        "Project external images (aligned face portraits) from `_in/photo` onto FFHQ model dlatent space. \n",
        "Results (found dlatent points as Numpy arrays in `*.npy` files, and video/still previews) are saved to `_out/proj` directory.  \n",
        "NB: first download [VGG model](https://drive.google.com/uc?id=1N2-m9qszOeVC9Tq77WxsLnuWwOedQiD2) and save it as `models/vgg/vgg16_zhang_perceptual.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJx9ws7n8APN"
      },
      "source": [
        "%run src/project_latent.py --model $model_pkl --in_dir _in/photo --out_dir _out/proj \r\n",
        "\r\n",
        "Image('_out/proj/blonde458/blonde458-1000.jpg', width=512, height=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uc1PVd0UllY"
      },
      "source": [
        "from IPython.display import Image\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JSDF8Tn8APN"
      },
      "source": [
        "Generate animation between saved dlatent points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIJJUfAN8APN"
      },
      "source": [
        "dlat = 'mynpy'\n",
        "path_in = '_in/' + dlat\n",
        "path_out = '_out/ffhq-' + dlat\n",
        "\n",
        "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10\n",
        "HTML(makevid(path_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBBc2CV8APN"
      },
      "source": [
        "> This loads saved dlatent points from `_in/mynpy` and produces smooth looped animation between them (with interpolation step of 50 frames). `mynpy` may be a file or a directory with `*.npy` or `*.npz` files. To select only few frames from a sequence `somename.npy`, create text file with comma-delimited frame numbers and save it as `somename.txt` in the same directory (check given examples for FFHQ model).\n",
        "\n",
        "Style-blending argument `--style_npy_file xxx.npy` would also load dlatent from `xxx.npy` and apply it to higher network layers. `--cubic` smoothing and `--digress X` displacements are also applicable here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J84NdtP38APO"
      },
      "source": [
        "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10 --style_npy_file _in/blonde458.npy --digress 2 --cubic\n",
        "HTML(makevid(path_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iykr7uKL8APO"
      },
      "source": [
        "Generate animation by moving saved dlatent point `_in/blonde458.npy` along feature direction vectors from `_in/vectors_ffhq` (aging/smiling/etc) one by one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UOToqrX68APO"
      },
      "source": [
        "%run src/_play_vectors.py --model $model_pkl --npy_file _in/blonde458.npy --vector_dir _in/vectors_ffhq --out_dir _out/ffhq_looks\n",
        "HTML(makevid('_out/ffhq_looks'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiItkMSM8APO"
      },
      "source": [
        "## Tweaking models\n",
        "\n",
        "NB: No real examples here! The commands are for reference, try with your own files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tldFcuR8APP"
      },
      "source": [
        "Strip G/D networks from a full model, leaving only Gs for inference. Resulting file is saved with `-Gs` suffix. It's recommended to add `-r` option to reconstruct the network, saving necessary arguments with it. Useful for foreign downloaded models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJwhP_IY8APP"
      },
      "source": [
        "%run src/model_convert.py --source snapshot-1024.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOS24WCS8APP"
      },
      "source": [
        "Add or remove layers (from a trained model) to adjust its resolution for further finetuning. This will produce new model with 512px resolution, populating weights on the layers up to 256px from the source snapshot (the rest will be initialized randomly). It also can decrease resolution (say, make 512 from 1024). Note that this effectively changes the number of layers in the model.   \n",
        "This option works with complete (G/D/Gs) models only, since it's purposed for transfer-learning (the resulting model will contain either partially random weights, or wrong `ToRGB` params). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gqba1BY8APP"
      },
      "source": [
        "%run src/model_convert.py --source snapshot-256.pkl --res 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUtk27qe8APP"
      },
      "source": [
        "Crop resolution of a trained model. This will produce working non-square 1024x768 model. Opposite to the method above, this one doesn't change layer count. This is experimental feature (as stated by the author @Aydao), also using some voluntary logic, so works only with basic resolutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKguO2Iw8APP"
      },
      "source": [
        "%run src/model_convert.py --source snapshot-1024.pkl --res 1024-768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJsXdF5P8APP"
      },
      "source": [
        "Combine lower layers from one model with higher layers from another. `<res>` is resolution, at which the models are switched (usually 32/64/128); `<level>` is 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJLnOqFQ8APQ"
      },
      "source": [
        "%run src/models_blend.py --pkl1 model1.pkl --pkl2 model2.pkl --res <res> --level <level>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRTS_bpI8APQ"
      },
      "source": [
        "Mix few models by stochastic averaging all weights. This would work properly only for models from one \"family\", i.e. uptrained (finetuned) from the same original model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRatWRXC8APQ"
      },
      "source": [
        "%run src/models_swa.py --in_dir <models_dir>"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}