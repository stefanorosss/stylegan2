{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN2 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules for notebook logging\n",
    "!pip install IPython\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare the data. If you work with patterns or shapes (rather than compostions), you can crop square fragments from bigger images (effectively multiplying their amount). \n",
    "Otherwise, edit images yourself as you wish, ensuring correct size (this also relates to non-square aspect ratios).\n",
    "\n",
    "Edit source and target paths below; `size` is fragment resolution, `step` is shift between the fragments. This will cut every source image into 512x512px fragments, overlapped with 256px shift by X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'data/src'\n",
    "data_dir = 'data/mydata'\n",
    "size = 512\n",
    "step = 256\n",
    "\n",
    "%run src/util/multicrop.py --in_dir $src_dir --out_dir $data_dir --size $size --step $step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make compact TFRecords dataset from directory with JPG images `data/mydata`. \n",
    "This will create `mydata-512x512.tfr` file in `data` directory.  \n",
    "*For images with alpha channel remove `--jpg` option.*  \n",
    "*For conditional model split the data by subfolders (`mydata/1`, `mydata/2`, ..) and add `--labels` option.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/training/dataset_tool.py --dataset $data_dir --jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train StyleGAN2 on the prepared dataset:  \n",
    "*(remove `--jpg_data` if your images have alpha channel)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --dataset $data_dir --jpg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run training process, according to the options in `src/train.py`. If there was no TFRecords file from the previous step, it will be created at this point. The training results (models and samples) are saved under the `train` directory, similar to original Nvidia approach. There are two types of models saved: compact (containing only Gs network for inference) as `<dataset>-...pkl` (e.g. `mydata-512-0360.pkl`), and full (containing G/D/Gs networks for further training) as `snapshot-...pkl`. \n",
    "\n",
    "By default, the most powerful SG2 config (F) is used; if you face OOM issue, you may resort to `--config E`, requiring less memory (with poorer results, of course). For small datasets (100x images instead of 10000x) one should add `--d_aug` option to use [Differential Augmentation](https://github.com/mit-han-lab/data-efficient-gans) for more effective training. \n",
    "\n",
    "The length of the training is defined by `--lod_step_kimg X` argument. It's kind of legacy from [progressive GAN](https://github.com/tkarras/progressive_growing_of_gans) and defines one step of progressive training. Network with base resolution 1024px will be trained for 20 such steps, for 512px - 18 steps, et cetera. Reasonable `lod_step_kimg` value for big datasets is 300-600, while in `--d_aug` mode 20-40 is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training process was interrupted, we can resume it from the last saved model as following:  \n",
    "*(replace `000-mydata-512-f` with existing training directory)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --dataset $data_dir --jpg_data --resume train/000-mydata-512-f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: In most cases it's much easier to use a \"transfer learning\" trick, rather than perform full training from the scratch. For that, we use existing well-trained model as a starter, and \"finetune\" (uptrain) it with our data. This works pretty well, even if our dataset is very different from the original model. \n",
    "\n",
    "So here is a faster way to train our GAN (presuming we have `ffhq-512.pkl` model already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/train.py --dataset $data_dir --jpg_data --resume train/ffhq-512.pkl --d_aug --lod_step_kimg 20 --finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no need to go for exact steps in this case, you may stop when you're ok with the results. Lower `lod_step_kimg` helps following the progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Let's produce some imagery from the original cat model (download it from [here](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and put to `models` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Video\n",
    "\n",
    "model = 'models/stylegan2-cat-config-f' # without \".pkl\" extension\n",
    "model_pkl = model + '.pkl' # with \".pkl\" extension\n",
    "output = '_out/cats'\n",
    "frames = '50-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate some animation to test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model_pkl --out_dir $output --frames $frames\n",
    "\n",
    "Image(filename = output + '/000000.jpg') # show first frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlzqD3DEGzIH"
   },
   "source": [
    "* Here we loaded the model 'as is', and produced 50 frames in its natural resolution, interpolating between random latent space keypoints, with a step of 10 frames between keypoints.\n",
    "\n",
    "If you have `ffmpeg` installed, you can convert it into video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sequence = output + '/%06d.jpg'\n",
    "out_video = output + '.mp4'\n",
    "\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "\n",
    "Video(out_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now let's generate more custom animation. For that we omit model extension, so the script would load custom network, effectively enabling special features, e.g. arbitrary resolution (set by `--size` argument in `X-Y` format).  \n",
    "`--cubic` option changes linear interpolation to cubic for smoother animation (there is also `--gauss` option for additional smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --cubic\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run `ffmpeg` command above after each generation, if you want to check results in motion.  \n",
    "Adding `--save_lat` option will save all traversed dlatent points as Numpy array in `*.npy` file (useful for further curating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate more various imagery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 768-256 -n 3-1\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we get animated composition of 3 independent frames, blended together horizontally (like the image in the repo header). Argument `--splitfine X` controls boundary fineness (0 = smoothest/default, higher => thinner).  \n",
    "\n",
    "* Instead of frame splitting, we can load external mask from b/w image file (it also can be folder with file sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --size 400-300 --latmask _in/mask.jpg\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `--digress X` adds some funky displacements with X strength (by tweaking initial constant layer).  \n",
    "`--trunc X` controls truncation psi parameter (0 = boring, 1+ = weird). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_genSGAN2.py --model $model --out_dir $output --frames $frames --digress 2 --trunc 0.5\n",
    "Image(output+'/000000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration\n",
    "\n",
    "For these experiments download [FFHQ model](https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-ffhq-config-f.pkl) and save to `models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Video\n",
    "\n",
    "model = 'models/stylegan2-ffhq-config-f' # without \".pkl\" extension\n",
    "model_pkl = model + '.pkl' # with \".pkl\" extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Project external images (aligned face portraits) from `_in/photo` onto StyleGAN2 model dlatent space. \n",
    "Results (found dlatent points as Numpy arrays in `*.npy` files, and video/still previews) are saved to `_out/proj` directory.  \n",
    "NB: first download [VGG model](https://drive.google.com/uc?id=1N2-m9qszOeVC9Tq77WxsLnuWwOedQiD2) and save it as `models/vgg/vgg16_zhang_perceptual.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/project_latent.py --model $model_pkl --in_dir _in/photo --out_dir _out/proj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate animation between saved dlatent points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlat = 'mynpy'\n",
    "path_in = '_in/' + dlat\n",
    "path_out = '_out/ffhq-' + dlat\n",
    "out_sequence = path_out + '/%06d.jpg'\n",
    "out_video = path_out + '.mp4'\n",
    "\n",
    "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10\n",
    "Image(path_out+'/000000.jpg', width=512, height=512)\n",
    "\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "Video(out_video, width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This loads saved dlatent points from `_in/mynpy` and produces smooth looped animation between them (with interpolation step of 50 frames). `mynpy` may be a file or a directory with `*.npy` or `*.npz` files. To select only few frames from a sequence `somename.npy`, create text file with comma-delimited frame numbers and save it as `somename.txt` in the same directory (check given examples for FFHQ model).\n",
    "\n",
    "* Style-blending argument `--style_npy_file blonde458.npy` would also load dlatent from `blonde458.npy` and apply it to higher network layers. `--cubic` smoothing and `--digress X` displacements are also applicable here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/_play_dlatents.py --model $model --dlatents $path_in --out_dir $path_out --fstep 10 --style_npy_file _in/blonde458.npy --digress 2 --cubic\n",
    "!ffmpeg -y -v warning -i $out_sequence $out_video\n",
    "Video(out_video, width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate animation by moving saved dlatent point `_in/blonde458.npy` along feature direction vectors from `_in/vectors_ffhq` (aging/smiling/etc) one by one: (check preview window!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run src/_play_vectors.py --model $model_pkl --npy_file _in/blonde458.npy --vector_dir _in/vectors_ffhq\n",
    "\n",
    "!ffmpeg -y -v warning -i _out/ttt/%06d.jpg _out/ffhq-vectors.mp4\n",
    "Video('ffhq-vectors.mp4', width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking models\n",
    "\n",
    "NB: No real examples here! Just reference commands, try with your own files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip G/D networks from a full model, leaving only Gs for inference. Resulting file is saved with `-Gs` suffix. It's recommended to add `-r` option to reconstruct the network, saving necessary arguments with it. Useful for foreign downloaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/model_convert.py --source snapshot-1024.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add or remove layers (from a trained model) to adjust its resolution for further finetuning. This will produce new model with 512px resolution, populating weights on the layers up to 256px from the source snapshot (the rest will be initialized randomly). It also can decrease resolution (say, make 512 from 1024). Note that this effectively changes the number of layers in the model.   \n",
    "This option works with complete (G/D/Gs) models only, since it's purposed for transfer-learning (the resulting model will contain either partially random weights, or wrong `ToRGB` params). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/model_convert.py --source snapshot-256.pkl --res 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop resolution of a trained model. This will produce working non-square 1024x768 model. Opposite to the method above, this one doesn't change layer count. This is experimental feature (as stated by the author @Aydao), also using some voluntary logic, so works only with basic resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/model_convert.py --source snapshot-1024.pkl --res 1024-768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine lower layers from one model with higher layers from another. `<res>` is resolution, at which the models are switched (usually 32/64/128); `<level>` is 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/models_blend.py --pkl1 model1.pkl --pkl2 model2.pkl --res <res> --level <level>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix few models by stochastic averaging all weights. This would work properly only for models from one \"family\", i.e. uptrained (finetuned) from the same original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/models_swa.py --in_dir <models_dir>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
